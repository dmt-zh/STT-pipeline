setup:
  torch_threads: 1
  seed: 123

clearml:
  enable: true
  project_name: 'Whisper Kazakh'
  experiment_name: 'Baseline tuning'
  tags:
    - baseline
    - tuning
    - small

model:
  model_size: small
  model_name: openai/whisper
  language: kazakh
  task: transcribe
  cache_dir: ./models/origin_model
  prefix: small_kk


data:
  eval_fraction: 30
  filter_audio: false
  filter_text: false
  debug_samples: 3
  num_proc: 2
  noise:
    include: false
    file_path: ./data/noisy_audio.wav
    snr_levels: [10, 20]
    percentage: 0.1
  common_voice:
    include: true
    lang: kk
    path: fsicoli/common_voice_19_0
    split: train[:100]+test[:30]+validation[:30]
    cache_dir: ./data/common_voice
    num_proc: 1
  fleurs:
    include: true
    lang: kk_kz
    path: google/fleurs
    split: train[:100]+validation[:30]+test[:30]
    cache_dir: ./data/fleurs
    num_proc: 1
  custom:
    include: true
    path: ./data/custom_kk
    slice: :350
    shuffle: true
    seed: 44

lora_params:
  alpha: 96
  r: 48
  dropout: 0.05
  bias: none
  target_modules:
    - q_proj
    - k_proj 
    - v_proj
    - o_proj

training_args:
  do_train: true
  num_train_epochs: 1
  max_steps: 10
  save_steps: 5
  logging_steps: 5
  eval_steps: 5
  output_dir: ./models/tunes
  per_device_train_batch_size: 18
  per_device_eval_batch_size: 18
  gradient_accumulation_steps: 2
  learning_rate: 0.00005
  weight_decay: 0.01
  max_grad_norm: 5.0
  lr_scheduler_type: cosine
  warmup_steps:
  logging_strategy: steps
  logging_dir: ./models/logs
  logging_first_step: true
  eval_strategy: steps
  eval_on_start: true
  save_strategy: steps
  save_total_limit: 60
  fp16: true
  bf16: false
  remove_unused_columns: false
  load_best_model_at_end: false
  greater_is_better: false
  metric_for_best_model: loss
  optim: adamw_torch
  group_by_length: false
  label_names:
    - labels
  report_to: none
