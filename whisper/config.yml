clearml:
  project_name: 'Whisper Kazakh'
  experiment_name: 'Baseline tuning'
  tags:
    - baseline
    - tuning
    - small

setup:
  torch_threads: 1
  seed: 123

model:
  model_size: small
  model_name: openai/whisper
  task: transcribe

data:
  noise:
    include: true
    file_path: ./data/noisy_audio.wav
    snr_levels: [10, 20]
    percentage: 0.1
  common_voice:
    include: false
    lang: kk
    path: 'fsicoli/common_voice_19_0'
    split: 'train[:100]+test[:30]+validation[:30]'
    cache_dir: './data/common_voice'
    num_proc: 2
    slice: 500
  fleurs:
    include: false
    lang: kk_kz
    path: 'google/fleurs'
    split: 'train[:100]+validation[:30]+test[:30]'
    cache_dir: './data/fleurs'
    slice: 500 
    num_proc: 2

lora_params:
  alpha: 96
  r: 48
  dropout: 0.05
  bias: none
  target_modules:
    - q_proj
    - k_proj 
    - v_proj
    - o_proj

training_args:
  per_device_train_batch_size: 18
  gradient_accumulation_steps: 2
  per_device_eval_batch_size: 18
  gradient_checkpointing: true
  optim: paged_adamw_8bit
  save_total_limit: 60
  save_strategy: steps
  save_steps: 100
  logging_steps: 100
  learning_rate: 0.00002
  fp16: false
  bf16: true
  eval_steps: 500
  evaluation_strategy: steps
  eval_strategy: steps
  num_train_epochs: 4
  max_steps: -1
  weight_decay: 0.01
  warmup_steps: 1000
  group_by_length: true
  lr_scheduler_type: cosine
  max_grad_norm: 5.0
  report_to: none
  load_best_model_at_end: true
  greater_is_better: false
  metric_for_best_model: loss
  generation_max_length: 128
  remove_unused_columns: false
  label_names:
    - labels
