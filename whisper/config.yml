setup:
  torch_threads: 1
  seed: 123

clearml:
  enable: true
  project_name: 'Whisper Kazakh'
  experiment_name: 'Small tuning'
  tags:

model:
  model_size: small
  model_name: openai/whisper
  language: kk
  task: transcribe
  cache_dir: ./models/origin_model
  prefix: small_kk_250903
  merged_model_dir: ./models/merged
  ct2_model_dir: ./models/ct2_converted
  quantization: float16

data:
  eval_fraction: 2500
  filter_audio: false
  filter_text: false
  debug_samples: 3
  num_proc: 4
  disable_tqdm: false
  noise:
    include: false
    file_path: ./data/noisy_audio.wav
    snr_levels: [10, 20]
    percentage: 0.1
  common_voice:
    include: true
    lang: kk
    path: fsicoli/common_voice_19_0
    split: train+test+validation
    cache_dir: ./data/common_voice
    num_proc: 4
  fleurs:
    include: true
    lang: kk_kz
    path: google/fleurs
    split: train+validation+test[:300]
    cache_dir: ./data/fleurs
    num_proc: 4
  custom:
    include: true
    path: ./data/custom_kk
    slice:
    shuffle: true

lora_params:
  alpha: 96
  r: 48
  dropout: 0.05
  bias: none
  target_modules:
    - q_proj
    - k_proj 
    - v_proj

training_args:
  do_train: true
  num_train_epochs: 1
  max_steps: 7000
  save_steps: 250
  logging_steps: 250
  eval_steps: 250
  output_dir: ./models/tunes
  per_device_train_batch_size: 18
  per_device_eval_batch_size: 18
  gradient_accumulation_steps: 25
  learning_rate: 0.000075
  weight_decay: 0.01
  max_grad_norm: 5.0
  lr_scheduler_type: cosine
  warmup_steps: 500
  label_smoothing_factor: 0
  logging_strategy: steps
  logging_dir: ./models/logs
  logging_first_step: true
  eval_strategy: steps
  eval_on_start: true
  save_strategy: steps
  save_total_limit: 25
  fp16: true
  fp16_full_eval: true
  bf16: false
  bf16_full_eval: false
  remove_unused_columns: false
  load_best_model_at_end: false
  greater_is_better: false
  metric_for_best_model: loss
  optim: adamw_torch
  group_by_length: false
  label_names:
    - labels
  report_to: none
